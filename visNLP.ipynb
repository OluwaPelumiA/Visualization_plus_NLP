{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sessionId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userQuery</th>\n",
       "      <th>event</th>\n",
       "      <th>flowName</th>\n",
       "      <th>intentName</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzvep9d4jxq3fvwxc2qtp2d5hjzppece</td>\n",
       "      <td>2021-01-17 15:17:58.520</td>\n",
       "      <td></td>\n",
       "      <td>policy_details</td>\n",
       "      <td>policy_details</td>\n",
       "      <td>None</td>\n",
       "      <td>15:17:58</td>\n",
       "      <td>2021-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzlzdxcog9mah27rlf0txl51soofdolv</td>\n",
       "      <td>2021-01-18 13:03:51.475</td>\n",
       "      <td></td>\n",
       "      <td>Policy_4DigitVerify</td>\n",
       "      <td>Policy_4DigitVerify</td>\n",
       "      <td>None</td>\n",
       "      <td>13:03:51</td>\n",
       "      <td>2021-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzkhhe1ipmm1objrw4r7ln3okw6toc9e</td>\n",
       "      <td>2021-01-28 14:52:30.643</td>\n",
       "      <td></td>\n",
       "      <td>policy_details</td>\n",
       "      <td>policy_details</td>\n",
       "      <td>None</td>\n",
       "      <td>14:52:30</td>\n",
       "      <td>2021-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzfllkihdcufgy8w9z7v7rdz9e4miekd</td>\n",
       "      <td>2021-01-13 18:28:21.589</td>\n",
       "      <td></td>\n",
       "      <td>policy_details</td>\n",
       "      <td>policy_details</td>\n",
       "      <td>None</td>\n",
       "      <td>18:28:21</td>\n",
       "      <td>2021-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzchk2s5uk6jvkwne4fe7lfzrevl3n2y</td>\n",
       "      <td>2021-01-01 14:19:53.556</td>\n",
       "      <td>Hello.</td>\n",
       "      <td>None</td>\n",
       "      <td>policy_details</td>\n",
       "      <td>None</td>\n",
       "      <td>14:19:53</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sessionId               timestamp userQuery  \\\n",
       "0  zzvep9d4jxq3fvwxc2qtp2d5hjzppece 2021-01-17 15:17:58.520             \n",
       "1  zzlzdxcog9mah27rlf0txl51soofdolv 2021-01-18 13:03:51.475             \n",
       "2  zzkhhe1ipmm1objrw4r7ln3okw6toc9e 2021-01-28 14:52:30.643             \n",
       "3  zzfllkihdcufgy8w9z7v7rdz9e4miekd 2021-01-13 18:28:21.589             \n",
       "4  zzchk2s5uk6jvkwne4fe7lfzrevl3n2y 2021-01-01 14:19:53.556    Hello.   \n",
       "\n",
       "                 event             flowName intentName      time        date  \n",
       "0       policy_details       policy_details       None  15:17:58  2021-01-17  \n",
       "1  Policy_4DigitVerify  Policy_4DigitVerify       None  13:03:51  2021-01-18  \n",
       "2       policy_details       policy_details       None  14:52:30  2021-01-28  \n",
       "3       policy_details       policy_details       None  18:28:21  2021-01-13  \n",
       "4                 None       policy_details       None  14:19:53  2021-01-01  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql import functions as F\n",
    "import pandasql as ps\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# load mongo data\n",
    "input_uri = \"mongodb://127.0.0.1/mydbname.mycollection\"\n",
    "output_uri = \"mongodb://127.0.0.1/mydbname.mycollection\"\n",
    "#dfs = pdm.read_mongo(\"mycollection\", [], \"mongodb://localhost:27017/mydbname\")\n",
    "\n",
    "my_spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"MyApp\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", input_uri)\\\n",
    "    .config(\"spark.mongodb.output.uri\", output_uri)\\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:2.4.2')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = my_spark.read.format('com.mongodb.spark.sql.DefaultSource').load()\n",
    "\n",
    "res = df.withColumn('id', row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)\n",
    "\n",
    "res.createOrReplaceTempView(\"temp\")\n",
    "features = my_spark.sql(\"SELECT id,sessionId,timestamp, userQuery, event,flowName,handled,intentName FROM temp\")\n",
    "\n",
    "q_time = features.withColumn('time', date_format('timestamp', 'HH:mm:ss'))\n",
    "#time and date added \n",
    "date_inclusive = q_time.withColumn('date', date_format('timestamp', 'yyyy-MM-dd'))\n",
    "userQuery = my_spark.sql(\"SELECT userQuery,timestamp FROM temp WHERE id IN (SELECT MAX(id) FROM temp GROUP BY userQuery) ORDER BY sessionId DESC;\")\n",
    "with_dateDF = userQuery.withColumn('date', date_format('timestamp', 'yyyy-MM-dd'))\n",
    "with_dateDF.toPandas().to_csv(\"query_files.csv\", header=True)\n",
    "\n",
    "#write entire data to csv\n",
    "#date_inclusive.toPandas().to_csv(\"sample_files.csv\", header=True)\n",
    "\n",
    "#date_inclusive.show(10)\n",
    "#filter rows using date...last 100 days \n",
    "#filter_date = date_inclusive.filter(F.col('date') <= F.date_sub(F.current_date(), 100))\n",
    "#filter_date.show(250)\n",
    "\n",
    "#q_time.show()\n",
    "\n",
    "#getting the last row of each sessionID\n",
    "feat3 = my_spark.sql(\"SELECT sessionId,timestamp, userQuery, event, flowName, intentName FROM temp WHERE id IN (SELECT MAX(id) FROM temp GROUP BY sessionId) ORDER BY sessionId DESC;\")\n",
    "feat3.createOrReplaceTempView(\"tabl\")\n",
    "groupDF = my_spark.sql(\"SELECT sessionId,timestamp, userQuery, event,flowName,intentName FROM tabl\")\n",
    "#groupDF = my_spark.sql(\"SELECT flowName, count(*) as N FROM tabl GROUP BY flowName\")\n",
    "q2_time = groupDF.withColumn('time', date_format('timestamp', 'HH:mm:ss'))\n",
    "date_inclusive2 = q2_time.withColumn('date', date_format('timestamp', 'yyyy-MM-dd'))\n",
    "#write entire data to csv\n",
    "date_inclusive2.toPandas().to_csv(\"analysis_data.csv\", header=True)\n",
    "#writing df to pandas\n",
    "pd_counts = date_inclusive2.toPandas()\n",
    "pd_counts.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-19 02:05:19.608766\n",
      "02-01-2021 14:00:00.000000\n",
      "                             sessionId               timestamp  \\\n",
      "1     zzlzdxcog9mah27rlf0txl51soofdolv 2021-01-18 13:03:51.475   \n",
      "2     zzkhhe1ipmm1objrw4r7ln3okw6toc9e 2021-01-28 14:52:30.643   \n",
      "6     zzc4akry8mjjzmysziv7u48uyib8ucfk 2021-01-19 15:11:48.051   \n",
      "7     zz9j56kd780bq8szgelcyhaehmtf6hri 2021-02-06 18:42:33.942   \n",
      "10    zyt5mo3a9kj1ilwh332jgqgo04l71awh 2021-01-20 13:35:30.698   \n",
      "...                                ...                     ...   \n",
      "7025  0174hj95m19tpc0rgff75f92k6ijwilu 2021-01-29 20:55:57.765   \n",
      "7026  00vgaemels355aa6c43hsn3maa1hdqtk 2021-01-18 14:47:08.042   \n",
      "7027  00oesptpfdz2qmqeu3i0zmzkkm9xwebz 2021-01-23 14:43:57.581   \n",
      "7030  00cma960xlqioes5iz36hbmnbidupj6q 2021-02-06 18:27:07.927   \n",
      "7032  008zu4maqohthhl4yaqwnx2xhtk4dwqk 2021-01-19 17:05:02.586   \n",
      "\n",
      "                                     userQuery                event  \\\n",
      "1                                               Policy_4DigitVerify   \n",
      "2                                                    policy_details   \n",
      "6                                                  workshop_details   \n",
      "7                                               Policy_4DigitVerify   \n",
      "10                                       11082                 None   \n",
      "...                                        ...                  ...   \n",
      "7025                                         1                 None   \n",
      "7026                                                 policy_details   \n",
      "7027                                             agenttransfer_flow   \n",
      "7030                                                 policy_details   \n",
      "7032  party mein bhai cal tha to piche se gadi                 None   \n",
      "\n",
      "                 flowName                   intentName      time       date  \n",
      "1     Policy_4DigitVerify                         None  13:03:51 2021-01-18  \n",
      "2          policy_details                         None  14:52:30 2021-01-28  \n",
      "6        workshop_details                         None  15:11:48 2021-01-19  \n",
      "7     Policy_4DigitVerify                         None  18:42:33 2021-02-06  \n",
      "10         policy_details                         None  13:35:30 2021-01-20  \n",
      "...                   ...                          ...       ...        ...  \n",
      "7025    Insured_name_flow                         None  20:55:57 2021-01-29  \n",
      "7026       policy_details                         None  14:47:08 2021-01-18  \n",
      "7027   agenttransfer_flow                         None  14:43:57 2021-01-23  \n",
      "7030       policy_details                         None  18:27:07 2021-02-06  \n",
      "7032  accidentDescription  accident_description_intent  17:05:02 2021-01-19  \n",
      "\n",
      "[3343 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "fd = pd_counts\n",
    "start_date = \"2021-01-18 13:03:51.475\"\n",
    "end_date = datetime.now()\n",
    "print(end_date)\n",
    "\n",
    "edate = datetime.strptime(\"2021-02-01T14:00:00.000-08:00\".split(\".\")[0],'%Y-%m-%dT%H:%M:%S').strftime(\"%m-%d-%Y %H:%M:%S.%f\")\n",
    "print(edate)\n",
    "#still need to check whether response is a date\n",
    "#filter data using date if day,days months years in query\n",
    "fd_filtered = fd.filter_date(\"timestamp\", start_date, end_date)\n",
    "print(fd_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_counts = date_inclusive2.toPandas()\n",
    "f = pd_counts.count()\n",
    "Total_number_of_calls = f[\"sessionId\"]\n",
    "#pd_countss = ps.sqldf(\"SELECT flowName, count(*) as N FROM pd_counts GROUP BY flowName\")\n",
    "#print(pd_countss.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of calls\n",
    "import pandas as pd\n",
    "unique_df = pd.read_csv(\"analysis_data.csv\")\n",
    "#unique_df.head()\n",
    "#unique_df[\"sessionId\"].count\n",
    "f = unique_df.count()\n",
    "print(f[\"sessionId\"])\n",
    "#determine failed and succesful calls\n",
    "g = unique_df.set_index([\"flowName\", \"sessionId\"]).count(level=\"flowName\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7033\n"
     ]
    }
   ],
   "source": [
    "#total number of calls\n",
    "import pandas as pd\n",
    "unique_df = pd.read_csv(\"analysis_data.csv\")\n",
    "#unique_df.head()\n",
    "#unique_df[\"sessionId\"].count\n",
    "f = unique_df.count()\n",
    "print(f[\"sessionId\"])\n",
    "#determine failed and succesful calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'body': '2pm yesterday', 'start': 0, 'value': {'values': [{'value': '2021-04-17T14:00:00.000-07:00', 'grain': 'hour', 'type': 'value'}], 'value': '2021-04-17T14:00:00.000-07:00', 'grain': 'hour', 'type': 'value'}, 'end': 13, 'dim': 'time', 'latent': False}]\n",
      "2021-04-17\n",
      "[{'entity': 'date', 'value': '04-17-2021 14:00:00.000000'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'date', 'value': '04-17-2021 14:00:00.000000'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def big_function(message):\n",
    "    exl_date_entities = [\"date\",\"time\"]\n",
    "    def extract_value(match):\n",
    "        if match[\"value\"].get(\"type\") == \"interval\":\n",
    "            value = {\"to\": match[\"value\"].get(\"to\", {}).get(\"value\"),\n",
    "                     \"from\": match[\"value\"].get(\"from\", {}).get(\"value\")}\n",
    "        else:\n",
    "            value = match[\"value\"].get(\"value\")\n",
    "\n",
    "        return value\n",
    "\n",
    "    def filter_irrelevant_matches(matches, requested_dimensions):\n",
    "        \"\"\"Only return dimensions the user configured\"\"\"\n",
    "\n",
    "        if requested_dimensions:\n",
    "            return [match\n",
    "                    for match in matches\n",
    "                    if match[\"dim\"] in requested_dimensions]\n",
    "        else:\n",
    "            return matches\n",
    "\n",
    "\n",
    "    def convert_duckling_format_to_rasa(matches):\n",
    "        extracted = []\n",
    "\n",
    "        for match in matches:\n",
    "            value = extract_value(match)\n",
    "            entity = {\"start\": match[\"start\"],\n",
    "                      \"end\": match[\"end\"],\n",
    "                      \"text\": match.get(\"body\", match.get(\"text\", None)),\n",
    "                      \"value\": value,\n",
    "                      \"confidence\": 1.0,\n",
    "                      \"additional_info\": match[\"value\"],\n",
    "                      \"entity\": match[\"dim\"]}\n",
    "\n",
    "            extracted.append(entity)\n",
    "\n",
    "        return extracted\n",
    "\n",
    "    def convert_duckling_date_format_to_exl(extracted):\n",
    "\n",
    "        entity_name = \"date\"\n",
    "        modified = []\n",
    "        for i in extracted:\n",
    "            if i[\"entity\"] == \"time\":\n",
    "                if type(i[\"value\"]) is dict:\n",
    "                    if i[\"value\"][\"from\"]:\n",
    "                        sdate = datetime.strptime(i[\"value\"][\"from\"].split(\".\")[0],\n",
    "                                                  '%Y-%m-%dT%H:%M:%S').strftime(\"%m-%d-%Y %H:%M:%S.%f\")\n",
    "                        modified.append({\"entity\": entity_name,\n",
    "                            \"value\":sdate})\n",
    "                    if i[\"value\"][\"to\"]:\n",
    "                        edate = datetime.strptime(i[\"value\"][\"to\"].split(\".\")[0],\n",
    "                           '%Y-%m-%dT%H:%M:%S').strftime(\"%m-%d-%Y %H:%M:%S.%f\")\n",
    "                        modified.append({\"entity\": entity_name,\n",
    "                            \"value\":edate})\n",
    "                else:\n",
    "                    if i[\"value\"]:\n",
    "                        print(i[\"value\"].split(\"T\")[0])\n",
    "                        date = datetime.strptime(i[\"value\"].split(\".\")[0],\n",
    "                            '%Y-%m-%dT%H:%M:%S').strftime(\"%m-%d-%Y %H:%M:%S.%f\")\n",
    "                        modified.append({\"entity\": entity_name,\n",
    "                            \"value\":date})\n",
    "            \n",
    "            else:\n",
    "                modified.append(i)\n",
    "        return modified\n",
    "\n",
    "    component_config = {\n",
    "        # by default all dimensions recognized by duckling are returned\n",
    "        # dimensions can be configured to contain an array of strings\n",
    "        # with the names of the dimensions to filter for\n",
    "        \"dimensions\": None,\n",
    "\n",
    "        # http url of the running duckling server\n",
    "        \"url\": None,\n",
    "\n",
    "        # locale - if not set, we will use the language of the model\n",
    "        \"locale\": None,\n",
    "\n",
    "        # timezone like Europe/Berlin\n",
    "        # if not set the default timezone of Duckling is going to be used\n",
    "        \"timezone\": None\n",
    "    }\n",
    "\n",
    "    def _locale():\n",
    "        if not component_config.get(\"locale\"):\n",
    "            # this is king of a quick fix to generate a proper locale\n",
    "            # works most of the time\n",
    "            language = \"en\"\n",
    "            locale_fix = \"{}_{}\".format(language,language.upper())\n",
    "            component_config[\"locale\"] = locale_fix\n",
    "        return component_config.get(\"locale\")\n",
    "\n",
    "    def _url():\n",
    "        url = \"http://localhost:8000\"\n",
    "        return url\n",
    "\n",
    "    def _payload(text, reference_time):\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"locale\": _locale(),\n",
    "            \"tz\": component_config.get(\"timezone\"),\n",
    "            \"reftime\": reference_time\n",
    "        }\n",
    "\n",
    "    def _duckling_parse(text, reference_time):\n",
    "        \"\"\"Sends the request to the duckling server and parses the result.\"\"\"\n",
    "        #text = text.replace(\"this summer\",\"december\")\n",
    "        #text = text.replace(\"flower\",\"december\")\n",
    "        #print(\"text is.....\",text)\n",
    "        try:\n",
    "            payload = _payload(text, reference_time)\n",
    "            headers = {\"Content-Type\": \"application/x-www-form-urlencoded; \"\n",
    "                                       \"charset=UTF-8\"}\n",
    "            response = requests.post(_url() + \"/parse\",\n",
    "                                     data=payload,\n",
    "                                     headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return simplejson.loads(response.text)\n",
    "            else:\n",
    "                logger.error(\"Failed to get a proper response from remote \"\n",
    "                             \"duckling. Status Code: {}. Response: {}\"\n",
    "                             \"\".format(response.status_code, response.text))\n",
    "                return []\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            logger.error(\"Failed to connect to duckling http server. Make sure \"\n",
    "                         \"the duckling server is running and the proper host \"\n",
    "                         \"and port are set in the configuration. More \"\n",
    "                         \"information on how to run the server can be found on \"\n",
    "                         \"github: \"\n",
    "                         \"https://github.com/facebook/duckling#quickstart \"\n",
    "                         \"Error: {}\".format(e))\n",
    "            return []\n",
    "\n",
    "    #@staticmethod\n",
    "    def _reference_time_from_message(message):\n",
    "        if message.time is not None:\n",
    "            try:\n",
    "                return int(message.time) * 1000\n",
    "            except ValueError as e:\n",
    "                logging.warning(\"Could not parse timestamp {}. Instead \"\n",
    "                                \"current UTC time will be passed to \"\n",
    "                                \"duckling. Error: {}\".format(message.time, e))\n",
    "        # fallbacks to current time, multiplied by 1000 because duckling\n",
    "        # requires the reftime in miliseconds\n",
    "        return int(time.time()) * 1000\n",
    "\n",
    "    if _url() is not None:\n",
    "        #message = \"yesterday\"\n",
    "        #reference_time = _reference_time_from_message(message)\n",
    "        # function to convert words to numbers\n",
    "        reference_time = None\n",
    "        msg = message.replace(\".\",\"\")\n",
    "        #print(\"msg is ..............\",msg)\n",
    "        #message_text = self.convert_message_text(mesg)\n",
    "        #print(\"the message text is.......\",message_text)\n",
    "        matches = _duckling_parse(msg, reference_time)\n",
    "        #print(\"matches ..................\",matches)\n",
    "        dimensions = component_config[\"dimensions\"]\n",
    "        relevant_matches = filter_irrelevant_matches(matches, dimensions)\n",
    "        print(relevant_matches)\n",
    "        extracted = convert_duckling_format_to_rasa(relevant_matches)\n",
    "        #print(\"extracted..........................\",extracted)\n",
    "        dextracted = convert_duckling_date_format_to_exl(extracted)\n",
    "        print(dextracted)\n",
    "    else:\n",
    "        dextracted = []\n",
    "        logger.warning(\"Duckling HTTP component in pipeline, but no \"\n",
    "                       \"`url` configuration in the config \"\n",
    "                       \"file nor is `RASA_DUCKLING_HTTP_URL` \"\n",
    "                       \"set as an environment variable.\")\n",
    "    return dextracted\n",
    "\n",
    "big_function(\"2pm yesterday\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yea\n"
     ]
    }
   ],
   "source": [
    "pd_c = pd_countss\n",
    "testing = pd_countss[\"flowName\"]\n",
    "testing2 = pd_countss[\"N\"]\n",
    "title = [ each for each in testing]\n",
    "counts = [ each for each in testing2]\n",
    "\n",
    "if \"generateClaimNumber\" in title:\n",
    "    print(\"yea\")\n",
    "    indx = title.index(\"generateClaimNumber\")\n",
    "    TKU_indx = title.index(\"Thank You FLow\")\n",
    "    number_tku = counts[TKU_indx]\n",
    "    number = counts[indx]\n",
    "    num = number + number_tku\n",
    "    pd_c.loc[-1] = [\"Successful calls\",num]\n",
    "    pd_c.index = pd_c.index + 1  # shifting index\n",
    "    pd_c = pd_c.sort_index()\n",
    "        \n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_c.to_csv(\"count_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nl4dv import NL4DV\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import simplejson\n",
    "def number_vis(message,pd_countss):\n",
    "    #print(pd_countss)\n",
    "    #determine failed and succesful calls\n",
    "    try:\n",
    "        pd_countsss = pd_countss.dropna()\n",
    "        title = [ each for each in pd_countsss['flowName']]\n",
    "        counts = [ each for each in pd_countsss['N']]\n",
    "        if \"generateClaimNumber\" in title:\n",
    "            print(\"yea\")\n",
    "            indx = title.index(\"generateClaimNumber\")\n",
    "            TKU_indx = title.index(\"Thank You FLow\")\n",
    "            number_tku = counts[TKU_indx]\n",
    "            number = counts[indx]\n",
    "            num = number + number_tku\n",
    "            pd_countsss.loc[-1] = [\"Successful calls\",num]\n",
    "            pd_countsss.index = pd_countsss.index + 1  # shifting index\n",
    "            pd_countsss = pd_countsss.sort_index()\n",
    "\n",
    "        else:\n",
    "            print(\"no\")\n",
    "\n",
    "        text =  [ each for each in pd_countsss['flowName']]\n",
    "        counts = [ each for each in pd_countsss['N']]\n",
    "        def remove_noise_punct(text):\n",
    "            #text = text.translate(str.maketrans('', '', punctuation))\n",
    "            for char in punctuation:\n",
    "                text = text.replace(char, ' ')\n",
    "            return text\n",
    "        textss = []\n",
    "        for i in text:\n",
    "            if i == \"agenttransfer_flow\":\n",
    "                i = \"agent transfer flow\"\n",
    "            else:\n",
    "                i = i\n",
    "            #print(i)\n",
    "            r = remove_noise_punct(i)\n",
    "            r =  re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", r)\n",
    "            textss.append(r)\n",
    "        print(textss)\n",
    "        num = []\n",
    "        for i in textss:\n",
    "            score = fuzz.partial_token_sort_ratio(message,i)\n",
    "            num.append(score)\n",
    "        maxi = max(num)\n",
    "        max_index = num.index(maxi)\n",
    "        numbers = counts[max_index]   \n",
    "        #print(textss[max_index])\n",
    "        #print(num)\n",
    "        #print(textss)\n",
    "        if \"how many\" in message:\n",
    "            #resp = message.strip(\"how many\")\n",
    "            resps = message.split(\"how many\")\n",
    "            resp = resps[1]\n",
    "            result = str(numbers) +\" \"+ resp\n",
    "            #print(result)\n",
    "            #print(\"nnnn\")\n",
    "        else:\n",
    "\n",
    "            if \"calls\" in message:\n",
    "                result = str(numbers) + \" \"+\"calls\"\n",
    "            else:\n",
    "                result = str(numbers) + \" in \"+ textss[max_index]\n",
    "                result = result\n",
    "            #print(result)\n",
    "    except:\n",
    "        result = \"I don't have records on the requested date\"\n",
    "        maxi = 0\n",
    "        pass\n",
    "    return result,maxi\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud():\n",
    "    vis = VegaLite({\n",
    "      \"$schema\": \"https://vega.github.io/schema/vega/v5.json\",\n",
    "      \"description\": \"A word cloud visualization of the user query.\",\n",
    "      \"width\": 800,\n",
    "      \"height\": 400,\n",
    "      \"padding\": 0,\n",
    "\n",
    "      \"data\": [\n",
    "        {\n",
    "          \"name\": \"table\",\n",
    "          \"url\": \"data1.json\",\n",
    "          \"transform\": [\n",
    "            {\n",
    "              \"type\": \"countpattern\",\n",
    "              \"field\": \"data\",\n",
    "              \"case\": \"upper\",\n",
    "              \"pattern\": \"[\\\\w']{3,}\",\n",
    "              \"stopwords\": \"(i|me|my|myself|we|us|our|ours|ourselves|you|your|yours|p.m|A.M|P.M|yourself|yourselves|he|him|his|himself|she|her|hers|herself|it|its|itself|they|them|their|theirs|themselves|what|which|who|whom|whose|this|that|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|will|would|should|can|could|ought|i'm|you're|he's|she's|it's|we're|they're|i've|you've|we've|they've|i'd|you'd|he'd|she'd|we'd|they'd|i'll|you'll|he'll|she'll|we'll|they'll|isn't|aren't|wasn't|weren't|hasn't|haven't|hadn't|doesn't|don't|didn't|won't|wouldn't|shan't|shouldn't|can't|cannot|couldn't|mustn't|let's|that's|who's|what's|here's|there's|when's|where's|why's|how's|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|upon|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|say|says|said|shall)\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"formula\", \"as\": \"angle\",\n",
    "              \"expr\": \"[-45, 0, 45][~~(random() * 3)]\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"formula\", \"as\": \"weight\",\n",
    "              \"expr\": \"if(datum.text=='VEGA', 600, 300)\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "\n",
    "      \"scales\": [\n",
    "        {\n",
    "          \"name\": \"color\",\n",
    "          \"type\": \"ordinal\",\n",
    "          \"domain\": {\"data\": \"table\", \"field\": \"text\"},\n",
    "          \"range\": [\"#d5a928\", \"#652c90\", \"#939597\"]\n",
    "        }\n",
    "      ],\n",
    "\n",
    "      \"marks\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"from\": {\"data\": \"table\"},\n",
    "          \"encode\": {\n",
    "            \"enter\": {\n",
    "              \"text\": {\"field\": \"text\"},\n",
    "              \"align\": {\"value\": \"center\"},\n",
    "              \"baseline\": {\"value\": \"alphabetic\"},\n",
    "              \"fill\": {\"scale\": \"color\", \"field\": \"text\"}\n",
    "            },\n",
    "            \"update\": {\n",
    "              \"fillOpacity\": {\"value\": 1}\n",
    "            },\n",
    "            \"hover\": {\n",
    "              \"fillOpacity\": {\"value\": 0.5}\n",
    "            }\n",
    "          },\n",
    "          \"transform\": [\n",
    "            {\n",
    "              \"type\": \"wordcloud\",\n",
    "              \"size\": [800, 400],\n",
    "              \"text\": {\"field\": \"text\"},\n",
    "              \"rotate\": {\"field\": \"datum.angle\"},\n",
    "              \"font\": \"Helvetica Neue, Arial\",\n",
    "              \"fontSize\": {\"field\": \"datum.count\"},\n",
    "              \"fontWeight\": {\"field\": \"datum.weight\"},\n",
    "              \"fontSizeRange\": [12, 56],\n",
    "              \"padding\": 2\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janitor\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "from datetime import date\n",
    "from string import punctuation\n",
    "from vega import VegaLite\n",
    "from nl4dv import NL4DV\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "parser = \"/home/tushar/spark/stanford-parser-full-2018-10-17/stanford-parser.jar\"\n",
    "model =  \"/home/tushar/spark/stanford-parser-full-2018-10-17/stanford-parser-3.9.2-models.jar\"\n",
    "\n",
    "def main_function(message):\n",
    "    message = str(message)\n",
    "    filenames = \"analysis_data.csv\" \n",
    "    rd = pd.read_csv(filenames)\n",
    "    file = \"query_files.csv\"\n",
    "    rd2 = pd.read_csv(file)\n",
    "    pd_countss = ps.sqldf(\"SELECT flowName, count(*) as N FROM pd_counts GROUP BY flowName\")\n",
    "    f = rd.count()\n",
    "    print(f[\"sessionId\"])\n",
    "    Total_number = f[\"sessionId\"]\n",
    "    pd_countss.loc[-1] = [\"Total number\",Total_number]\n",
    "    pd_countss.index = pd_countss.index + 1  # shifting index\n",
    "    pd_countss = pd_countss.sort_index()\n",
    "    #print(pd_countss.head())\n",
    "    try:\n",
    "        date_function= big_function(message)\n",
    "        start_date = date_function[0]['value']\n",
    "        end_date = datetime.now()\n",
    "        #still need to check whether response is a date\n",
    "        #filter data using date if day,days months years in query\n",
    "        df_filtered = rd.filter_date(\"date\", start_date, end_date)\n",
    "        #print(df_filtered)\n",
    "        df_filtered2 = rd2.filter_date(\"date\", start_date, end_date)\n",
    "        file = \"filtered_text.csv\"\n",
    "        df_filtered2.to_csv(file,header = True)\n",
    "\n",
    "        #write to csv \n",
    "        filenames = \"filtered_data.csv\"\n",
    "        df_filtered.to_csv(filenames,header = True)\n",
    "        pd_countss = ps.sqldf(\"SELECT flowName, count(*) as N FROM df_filtered GROUP BY flowName\")\n",
    "        f = df_filtered.count()\n",
    "        print(f[\"sessionId\"])\n",
    "        Total_number = f[\"sessionId\"]\n",
    "        pd_countss.loc[-1] = [\"Total number\",Total_number]\n",
    "        pd_countss.index = pd_countss.index + 1  # shifting index\n",
    "        pd_countss = pd_countss.sort_index()\n",
    "        \n",
    "        #print(pd_countss)\n",
    "        print(\"completed\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    pd_countss = pd_countss\n",
    "    #print(pd_countss)\n",
    "    response,threshold = number_vis(message, pd_countss)\n",
    "    print(threshold)\n",
    "\n",
    "    if \"create\" in message and threshold <= 65:\n",
    "            \n",
    "        #create .json synonym list\n",
    "        #change the nl4dv filename to created csv\n",
    "\n",
    "        data_url=filenames\n",
    "        print(data_url)\n",
    "        alias_url2 = \"nl4dv-master/examples/assets/aliases/data_alias.json\"\n",
    "        # -------------------- Special attributes / tokens ---------------------------\n",
    "        label_attribute = \"Title\"\n",
    "        ignore_words = ['movie']\n",
    "\n",
    "        # -------------------- Attribute Datatype Overrides ---------------------------\n",
    "        attribute_datatypes = {\"Release Year\": \"T\"}\n",
    "        # Initialize NL4DV and set the above configurations\n",
    "        dependency_parser_config = {'name': 'corenlp','model': model,'parser': parser}\n",
    "        nl4dv_instance = NL4DV(verbose=False)\n",
    "        nl4dv_instance.set_data(data_url=data_url)\n",
    "        nl4dv_instance.set_alias_map(alias_url=alias_url2)\n",
    "        nl4dv_instance.set_label_attribute(label_attribute=label_attribute)\n",
    "        nl4dv_instance.set_ignore_words(ignore_words=ignore_words)\n",
    "        #nl4dv_instance.set_attribute_datatype(attr_type_obj=attribute_datatypes)\n",
    "        nl4dv_instance.set_dependency_parser(config=dependency_parser_config)\n",
    "        query = message\n",
    "        print(query)\n",
    "        # -------------------- Ask Query ---------------------------\n",
    "        #nl4dv_response = nl4dv_instance.analyze_query(query, debug=True)\n",
    "        #print(\"\\nAttributes List:\")\n",
    "        #print(nl4dv_response['attributeMap'].keys())\n",
    "        response = nl4dv_instance.render_vis(query)\n",
    "    elif \"word\" in message:\n",
    "        print(message)\n",
    "        #file = \"query_files.csv\"\n",
    "        #try:\n",
    "        print(file)\n",
    "        data = pd.read_csv(file)\n",
    "        query = data['userQuery']\n",
    "\n",
    "        q = query.to_string(index = False)\n",
    "        if query.empty == True:\n",
    "            response = \" I don't have records on the requested date\"\n",
    "        else:\n",
    "            r = q.replace(\"NaN\",\"\")\n",
    "\n",
    "            res = \" \".join(r.split())\n",
    "            #f = r.replace(\" \",\"\")\n",
    "            texts = str(res)\n",
    "            text_without_number =  ''.join(filter(lambda item: not item.isdigit(), texts))\n",
    "\n",
    "            import json\n",
    "            with open('data1.json', 'w') as outfile:\n",
    "                json.dump(text_without_number, outfile)\n",
    "            print(\"done\")\n",
    "            response = word_cloud()\n",
    "            #except:\n",
    "            #\n",
    "            response = response\n",
    "        \n",
    "    return response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7033\n",
      "[]\n",
      "[]\n",
      "yea\n",
      "['Successful calls', 'Total number', 'Caller Details', 'Garage Name', 'Insured name flow', 'Insured name no intent', 'Insured name yes intent', 'Policy 4 Digit Verify', 'Thank You F Low', 'accident City', 'accident Date', 'accident Date no', 'accident Date yes', 'accident Description', 'accident Time', 'agent transfer flow', 'ask email', 'check rmn', 'driver Name', 'garage City yes', 'garage Pincode yes', 'garage city', 'garage pincode', 'generate Claim Number', 'insured Email no', 'insured Email yes', 'insured Number no', 'insured Number yes', 'insured email', 'insured number', 'insured number verify', 'intimator number', 'policy details', 'vehicle Number no', 'vehicle Number yes', 'vehicle number', 'weather', 'welcome', 'workshop details', 'workshop no', 'workshop yes']\n",
      "75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7033 calls'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_function(\"what is the total number of calls transfer to agent \")\n",
    "#how many calls got canceled at email address\n",
    "#show wordcloud of user query in last 2 months\n",
    "#how many calls was transfered to agent since 29th of december 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah\n"
     ]
    }
   ],
   "source": [
    "messe = \"what was transfered to word cloud agent since 29th of december 2020\"\n",
    "if \"word\"  in messe:\n",
    "    print(\"yeah\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "number_vis() missing 1 required positional argument: 'pd_countss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-3e6d05ba1c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"can you tell me how many calls got canceled at email address\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnumber_vis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: number_vis() missing 1 required positional argument: 'pd_countss'"
     ]
    }
   ],
   "source": [
    "message = \"can you tell me how many calls got canceled at email address\"\n",
    "number_vis(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
